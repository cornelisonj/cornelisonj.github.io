[
  {
    "path": "posts/welcome/",
    "title": "Webscraping With Python",
    "description": "Python can be a great tool for webscraping.",
    "author": [
      {
        "name": "John Cornelison",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [],
    "contents": "\r\nThe Packages\r\nThere are three packages that help with scraping websites using python, Beautiful Soup Selenium, and Scrapy.\r\nBeautiful Soup\r\nIf the webscraping you plan on doing is small, then Beautiful Soup is very useful. Beautriful Soup helps you get started scraping right away.\r\nThere are different ways to webscrap sites. For example, in r, I like to use the rvest package the css selector google chrome extention. Beautiful soup on the other hand, scrapes by parsing directly from the HTML.\r\n\r\n\r\nhtml_doc = \"\"\"\r\n<html><head><title>The Dormouse's story<\/title><\/head>\r\n<body>\r\n<p class=\"title\"><b>The Dormouse's story<\/b><\/p>\r\n\r\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\r\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie<\/a>,\r\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie<\/a> and\r\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie<\/a>;\r\nand they lived at the bottom of a well.<\/p>\r\n\r\n<p class=\"story\">...<\/p>\r\n\"\"\"\r\n\r\n\r\n\r\nfrom bs4 import BeautifulSoup\r\nsoup = BeautifulSoup(html_doc, 'html.parser')\r\n\r\nprint(soup.prettify())\r\n<html>\r\n <head>\r\n  <title>\r\n   The Dormouse's story\r\n  <\/title>\r\n <\/head>\r\n <body>\r\n  <p class=\"title\">\r\n   <b>\r\n    The Dormouse's story\r\n   <\/b>\r\n  <\/p>\r\n  <p class=\"story\">\r\n   Once upon a time there were three little sisters; and their names were\r\n   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\r\n    Elsie\r\n   <\/a>\r\n   ,\r\n   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\r\n    Lacie\r\n   <\/a>\r\n   and\r\n   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\r\n    Tillie\r\n   <\/a>\r\n   ;\r\nand they lived at the bottom of a well.\r\n  <\/p>\r\n  <p class=\"story\">\r\n   ...\r\n  <\/p>\r\n <\/body>\r\n<\/html>\r\n\r\n\r\n\r\nprint(soup.title)\r\n<title>The Dormouse's story<\/title>\r\nprint(soup.title.name)\r\ntitle\r\nprint(soup.title.string)\r\nThe Dormouse's story\r\nprint(soup.title.parent.name)\r\nhead\r\nprint(soup.p)\r\n<p class=\"title\"><b>The Dormouse's story<\/b><\/p>\r\nprint(soup.p['class'])\r\n['title']\r\nprint(soup.a)\r\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie<\/a>\r\nprint(soup.find(id=\"link3\"))\r\n<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie<\/a>\r\n\r\nSelenium\r\nIf the websites you are scraping needs logins, then Selenium is very useful. Selenium acts as a bot to go through the login and allow you to start scraping the websites that require authentification\r\nI did not use Selenium during my senior project, so I will just put a link to Selenium’s documentation website. Click here\r\nScrapy\r\nScrapy is my personal favorite. Scrapy is not as user friendly as the other packages, but their is so much that you can do with Scrapy. Below is an example of using a spider to scrape quotes from http://quotes.toscrape.com.\r\n\r\n\r\nimport scrapy\r\n\r\n\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = 'quotes'\r\n    start_urls = [\r\n        'http://quotes.toscrape.com/tag/humor/',\r\n    ]\r\n\r\n    def parse(self, response):\r\n        for quote in response.css('div.quote'):\r\n            yield {\r\n                'author': quote.xpath('span/small/text()').get(),\r\n                'text': quote.css('span.text::text').get(),\r\n            }\r\n\r\n        next_page = response.css('li.next a::attr(\"href\")').get()\r\n        if next_page is not None:\r\n            yield response.follow(next_page, self.parse)\r\n\r\nThe code above needs to be saved as a .py file. I named it quotes_spider. Once the file is saved, to run the spider, we use the following code in the command terminal.\r\nscrapy runspider quotes_spider.py -o quotes.jl\r\nThis scrapes the author and text of the quotes. If there is another page, it will scrape the same information from it. It then stores the information we scraped in a JSON Lines format file called quotes.\r\nthe contents of the quotes.jl file should look like this…\r\n\r\n“{”author“:”Jane Austen“,”text“:”\r\n\r\n\r\n{“author”: “Steve Martin”, “text”: \"\r\n\r\n\r\n{“author”: “Garrison Keillor”, “text”: \"\r\n\r\nThere is too much to Scrapy for me to cover it all here. If you have access to LinkedIn Learning, they have a wonderful tutorial on webscraping with Scrapy. Click here\r\nI hope you enjoy your next webscraping adventure!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-07T14:24:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-03-yes-i-have-a-blog/",
    "title": "Google Apps Script",
    "description": "Google Apps Script is a rapid application development plateform that is used to make custom solutions with Google Workspace. I used it with Google Spreadsheets",
    "author": [
      {
        "name": "John Cornelison",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\r\nOverview\r\nDuring my senior project, I worked with Google Apps Script to finish/fix a custom tool used to upload data from google spreadsheets directly into Big Query.\r\nBig Query\r\nReal quick, Big Query is a part of Google Cloud that stores the data warehouses, data sets, and data tables. It is also where one can create query’s to view, create, delete, alter and change the data warehouse, data sets, and data tables.\r\nGoogle Apps Script\r\nNow back to Google Apps Script. Essentially, Google Apps Script is JavaScript with built in methods and classes to work with Google Products. Like mentioned above, I used Google Apps Script to build a tool that could take a google spreadsheet filled with data, select the columns, change the column headers, and store it in Big Query.\r\nalt text\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-08T15:55:48-05:00",
    "input_file": "yes-i-have-a-blog.knit.md"
  }
]
